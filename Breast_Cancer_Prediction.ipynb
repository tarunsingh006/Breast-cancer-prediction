{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Predictive Analysis To Predict Diagnosis of a Breast Tumor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9774957-e9a8-4e5d-a3a2-11d58c98f4f3"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part_1: Problem Statement\n",
    "Breast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a result of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound, and biopsy are commonly used to diagnose breast cancer performed.\n",
    "\n",
    "## 1.1 Expected outcome\n",
    "Given breast cancer results from breast fine-needle aspiration (FNA) test (is a quick and simple procedure to perform, which removes some fluid or cells from a breast lesion or cyst (a lump, sore, or swelling) with a fine needle similar to a blood sample needle). Since this build a model that can classify a breast cancer tumor using two training classification:\n",
    "* 1 = Malignant (Cancerous) - Present\n",
    "* 0 = Benign (Not Cancerous) -Absent\n",
    "\n",
    "## 1.2 Objective \n",
    "Since the labels in the data are discrete, the predication falls into two categories, (i.e. Malignant or benign). In machine learning, this is a classification problem. \n",
    "        \n",
    "> *Thus, the goal is to classify whether the breast cancer is benign or malignant and predict the recurrence and non-recurrence of malignant cases after a certain period.  To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.*\n",
    "\n",
    "## 1.3 Identify data sources\n",
    "The **[Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)** datasets is available as machine learning repository maintained by the University of California, Irvine. The dataset contains **569 samples of malignant and benign tumor cells**. \n",
    "* The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M = malignant, B = benign), respectively. \n",
    "* The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant. \n",
    "\n",
    "### Getting Started: Load libraries and set options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:45.940170Z",
     "start_time": "2021-12-06T08:41:44.770468Z"
    },
    "nbpresent": {
     "id": "e7a5c9bd-eaa8-4f5e-9e19-18ceea5e11af"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import numpy as np         # linear algebra\n",
    "import pandas as pd        # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Read the file \"data.csv\" and print the contents.\n",
    "df = pd.read_csv('data/data.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "818004ce-3b93-48be-9fb8-9bacb0f07f33"
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "First, load the supplied CSV file using additional options in the Pandas **`read_csv`** function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b6af97c7-30d4-4fc6-b9d5-a213f70788c2"
    }
   },
   "source": [
    "### Inspecting the data\n",
    "The first step is to visually inspect the new data set. There are multiple ways to achieve this:\n",
    "* The easiest being to request the first few records using the DataFrame **`data.head()`** method. By default, **`data.head()`** returns the first 5 rows from the DataFrame object df (excluding the header row). \n",
    "* Alternatively, one can also use **`df.tail()`** to return the five rows of the data frame. \n",
    "* For both head and  tail methods, there is an option to specify the number of records by including the required number in between the parentheses when calling either method.Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:45.997235Z",
     "start_time": "2021-12-06T08:41:45.942172Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:46.136154Z",
     "start_time": "2021-12-06T08:41:46.003233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the cleaner version of dataframe with \"id\" for future analyis\n",
    "df.to_csv('data/data_clean_id.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dd80c7bb-5dfd-4bac-b1f9-92f09954a04d"
    }
   },
   "source": [
    "You can check the number of cases, as well as the number of fields, using the shape method, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:46.229103Z",
     "start_time": "2021-12-06T08:41:46.141153Z"
    },
    "nbpresent": {
     "id": "678c0d45-faba-4084-9235-0923c653792a"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38           122.8     1001.0   \n",
       "1         M        20.57         17.77           132.9     1326.0   \n",
       "2         M        19.69         21.25           130.0     1203.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33            184.6   \n",
       "1         0.1812  ...         24.99          23.41            158.8   \n",
       "2         0.2069  ...         23.57          25.53            152.5   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Id column is redundant and not useful, we want to drop it\n",
    "df.drop('id', axis =1, inplace=True)\n",
    "# df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:46.322049Z",
     "start_time": "2021-12-06T08:41:46.233101Z"
    },
    "nbpresent": {
     "id": "0a72bf45-3489-4633-a99f-d4fba5d072ac"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "01bd515a-1951-4ae7-8d39-a96079fc9eff"
    }
   },
   "source": [
    "In the result displayed, you can see the data has 569 records, each with 32 columns.\n",
    "\n",
    "The **`info()`** method provides a concise summary of the data; from the output, it provides the type of data in each column, the number of non-null values in each column, and how much memory the data frame is using.\n",
    "\n",
    "The method **`get_dtype_counts()`** will return the number of columns of each type in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:46.445978Z",
     "start_time": "2021-12-06T08:41:46.333042Z"
    },
    "nbpresent": {
     "id": "e6f8be49-68cc-4381-9b0f-c872712045b6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   diagnosis                569 non-null    object \n",
      " 1   radius_mean              569 non-null    float64\n",
      " 2   texture_mean             569 non-null    float64\n",
      " 3   perimeter_mean           569 non-null    float64\n",
      " 4   area_mean                569 non-null    float64\n",
      " 5   smoothness_mean          569 non-null    float64\n",
      " 6   compactness_mean         569 non-null    float64\n",
      " 7   concavity_mean           569 non-null    float64\n",
      " 8   concave points_mean      569 non-null    float64\n",
      " 9   symmetry_mean            569 non-null    float64\n",
      " 10  fractal_dimension_mean   569 non-null    float64\n",
      " 11  radius_se                569 non-null    float64\n",
      " 12  texture_se               569 non-null    float64\n",
      " 13  perimeter_se             569 non-null    float64\n",
      " 14  area_se                  569 non-null    float64\n",
      " 15  smoothness_se            569 non-null    float64\n",
      " 16  compactness_se           569 non-null    float64\n",
      " 17  concavity_se             569 non-null    float64\n",
      " 18  concave points_se        569 non-null    float64\n",
      " 19  symmetry_se              569 non-null    float64\n",
      " 20  fractal_dimension_se     569 non-null    float64\n",
      " 21  radius_worst             569 non-null    float64\n",
      " 22  texture_worst            569 non-null    float64\n",
      " 23  perimeter_worst          569 non-null    float64\n",
      " 24  area_worst               569 non-null    float64\n",
      " 25  smoothness_worst         569 non-null    float64\n",
      " 26  compactness_worst        569 non-null    float64\n",
      " 27  concavity_worst          569 non-null    float64\n",
      " 28  concave points_worst     569 non-null    float64\n",
      " 29  symmetry_worst           569 non-null    float64\n",
      " 30  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), object(1)\n",
      "memory usage: 137.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Review data types with \"info()\".\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:46.521933Z",
     "start_time": "2021-12-06T08:41:46.448975Z"
    },
    "nbpresent": {
     "id": "73ab9300-46cb-4deb-a673-7b6f0ea718bc"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                  False\n",
       "radius_mean                False\n",
       "texture_mean               False\n",
       "perimeter_mean             False\n",
       "area_mean                  False\n",
       "smoothness_mean            False\n",
       "compactness_mean           False\n",
       "concavity_mean             False\n",
       "concave points_mean        False\n",
       "symmetry_mean              False\n",
       "fractal_dimension_mean     False\n",
       "radius_se                  False\n",
       "texture_se                 False\n",
       "perimeter_se               False\n",
       "area_se                    False\n",
       "smoothness_se              False\n",
       "compactness_se             False\n",
       "concavity_se               False\n",
       "concave points_se          False\n",
       "symmetry_se                False\n",
       "fractal_dimension_se       False\n",
       "radius_worst               False\n",
       "texture_worst              False\n",
       "perimeter_worst            False\n",
       "area_worst                 False\n",
       "smoothness_worst           False\n",
       "compactness_worst          False\n",
       "concavity_worst            False\n",
       "concave points_worst       False\n",
       "symmetry_worst             False\n",
       "fractal_dimension_worst    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing variables\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:46.614884Z",
     "start_time": "2021-12-06T08:41:46.526933Z"
    },
    "nbpresent": {
     "id": "5f0c338b-0d11-4d51-836a-e003c07412ee"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diagnosis.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a043ba25-f180-41cf-b29b-ec7e6bab8d57"
    }
   },
   "source": [
    "From the results above, diagnosis is a categorical variable, because it represents a fix number of possible values (i.e, Malignant, of Benign. The machine learning algorithms wants numbers, and not strings, as their inputs so we need some method of coding to convert them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:46.739828Z",
     "start_time": "2021-12-06T08:41:46.619879Z"
    },
    "nbpresent": {
     "id": "06fb5119-7446-4892-a2a5-4770b4f98297"
    }
   },
   "outputs": [],
   "source": [
    "# Save the cleaner version of dataframe for future analyis\n",
    "df.to_csv('data/data_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbpresent": {
     "id": "9469d4ac-470e-4b21-a460-bdfb54fd0d4f"
    }
   },
   "source": [
    "> **NOTE:** Now that we have a good intuitive sense of the data, Next step involves taking a closer look at attributes and data values. In Part_2, we will explore the data further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "51d20db6-3dc0-4be6-9d8b-aacbe7a7e9ce"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: Exploratory Data Analysis\n",
    "\n",
    "Now that we have a good intuitive sense of the data, Next step involves taking a closer look at attributes and data values. In this section, I am getting familiar with the data, which will provide useful knowledge for data pre-processing.\n",
    "\n",
    "## 2.1 Objectives of Data Exploration\n",
    "Exploratory data analysis (EDA) is a very important step which takes place after feature engineering and acquiring data and it should be done before any modeling. This is because it is very important for a data scientist to be able to understand the nature of the data without making assumptions. The results of data exploration can be extremely useful in grasping the structure of the data, the distribution of the values, and the presence of extreme values and interrelationships within the data set.\n",
    "> **The purpose of EDA is:**\n",
    "* to use summary statistics and visualizations to better understand data, \n",
    "*find clues about the tendencies of the data, its quality and to formulate assumptions and the hypothesis of our analysis\n",
    "* For data preprocessing to be successful, it is essential to have an overall picture of your data\n",
    "Basic statistical descriptions can be used to identify properties of the data and highlight which data values should be treated as noise or outliers.** \n",
    "\n",
    "Next step is to explore the data. There are two approached used to examine the data using:\n",
    "\n",
    "1. ***Descriptive statistics*** is the process of condensing key characteristics of the data set into simple numeric metrics. Some of the common metrics used are mean, standard deviation, and correlation. \n",
    "\t\n",
    "2. ***Visualization*** is the process of projecting the data, or parts of it, into Cartesian space or into abstract images. In the data mining process, data exploration is leveraged in many different steps including preprocessing, modeling, and interpretation of results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e35d8ace-8b5e-4e17-9fc7-dabb08a9137e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.2 Descriptive statistics\n",
    "Summary statistics are measurements meant to describe data. In the field of descriptive statistics, there are many **[summary measurements](http://www.saedsayad.com/numerical_variables.htm)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:48.238775Z",
     "start_time": "2021-12-06T08:41:46.742807Z"
    },
    "nbpresent": {
     "id": "5fa5dce8-1b18-450b-8d58-3f35dd8e6662"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load libraries for data processing\n",
    "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns # data visualization\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15,8) \n",
    "plt.rcParams['axes.titlesize'] = 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:48.300747Z",
     "start_time": "2021-12-06T08:41:48.240777Z"
    },
    "nbpresent": {
     "id": "d852a8ab-9f20-4764-b01d-b3c75db047d4"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38           122.8     1001.0   \n",
       "1    842517         M        20.57         17.77           132.9     1326.0   \n",
       "2  84300903         M        19.69         21.25           130.0     1203.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33            184.6      2019.0   \n",
       "1  ...         24.99          23.41            158.8      1956.0   \n",
       "2  ...         23.57          25.53            152.5      1709.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usind clean data with \"id\"\n",
    "df = pd.read_csv('data/data_clean_id.csv', index_col=False)\n",
    "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:48.470649Z",
     "start_time": "2021-12-06T08:41:48.303746Z"
    },
    "nbpresent": {
     "id": "2cd780da-91da-4c16-8f26-3b2d07f3092a"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "count     569.000000  ...    569.000000     569.000000       569.000000   \n",
       "mean        0.181162  ...     16.269190      25.677223       107.261213   \n",
       "std         0.027414  ...      4.833242       6.146258        33.602542   \n",
       "min         0.106000  ...      7.930000      12.020000        50.410000   \n",
       "25%         0.161900  ...     13.010000      21.080000        84.110000   \n",
       "50%         0.179200  ...     14.970000      25.410000        97.660000   \n",
       "75%         0.195700  ...     18.790000      29.720000       125.400000   \n",
       "max         0.304000  ...     36.040000      49.540000       251.200000   \n",
       "\n",
       "        area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "count            569.000000      569.000000               569.000000  \n",
       "mean               0.114606        0.290076                 0.083946  \n",
       "std                0.065732        0.061867                 0.018061  \n",
       "min                0.000000        0.156500                 0.055040  \n",
       "25%                0.064930        0.250400                 0.071460  \n",
       "50%                0.099930        0.282200                 0.080040  \n",
       "75%                0.161400        0.317900                 0.092080  \n",
       "max                0.291000        0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:48.501631Z",
     "start_time": "2021-12-06T08:41:48.473648Z"
    },
    "nbpresent": {
     "id": "205bed2d-75b6-4266-8e59-79997c15398e"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "could not convert string to float: 'M'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\nanops.py:85\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\nanops.py:477\u001b[0m, in \u001b[0;36mmaybe_operate_rowwise.<locals>.newfunc\u001b[1;34m(values, axis, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(results)\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(values, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\nanops.py:1240\u001b[0m, in \u001b[0;36mnanskew\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1240\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1241\u001b[0m     count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'M'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskew\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\frame.py:11768\u001b[0m, in \u001b[0;36mDataFrame.skew\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11760\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskew\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m  11761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mskew\u001b[39m(\n\u001b[0;32m  11762\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11766\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11767\u001b[0m ):\n\u001b[1;32m> 11768\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mskew(axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m  11769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[0;32m  11770\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskew\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\generic.py:12461\u001b[0m, in \u001b[0;36mNDFrame.skew\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mskew\u001b[39m(\n\u001b[0;32m  12455\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  12456\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12459\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  12460\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 12461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  12462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskew\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanskew, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  12463\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\generic.py:12396\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12392\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[0;32m  12394\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m> 12396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  12397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  12398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\frame.py:11569\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  11565\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m  11567\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  11568\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 11569\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11570\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  11571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:406\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 406\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    409\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\frame.py:11488\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m  11486\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[0;32m  11487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m> 11488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_projects\\lib\\site-packages\\pandas\\core\\nanops.py:92\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 92\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: could not convert string to float: 'M'"
     ]
    }
   ],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "230b9836-0a87-4484-986e-c9deccaa5b94"
    }
   },
   "source": [
    " >The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew.\n",
    " From the graphs, we can see that **`radius_mean`**, **`perimeter_mean`**, **`area_mean`**, **`concavity_mean`** and **`concave_points_mean`** are useful in predicting cancer type due to the distinct grouping between malignant and benign cancer types in these features. We can also see that **`area_worst`** and **`perimeter_worst`** are also quite useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:48.577589Z",
     "start_time": "2021-12-06T08:41:48.506630Z"
    },
    "nbpresent": {
     "id": "474800c5-bebb-4550-8beb-4186520dd457"
    }
   },
   "outputs": [],
   "source": [
    "df.diagnosis.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:48.716985Z",
     "start_time": "2021-12-06T08:41:48.580588Z"
    },
    "nbpresent": {
     "id": "44abff06-48b5-40d9-9b9a-8a102086eb35"
    }
   },
   "outputs": [],
   "source": [
    "# Group by diagnosis and review the output.\n",
    "diag_gr = df.groupby('diagnosis', axis=0)\n",
    "pd.DataFrame(diag_gr.size(), columns=['# of observations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "589f5732-9b8e-4f20-a095-6d816e40f6b4"
    }
   },
   "source": [
    "Check binary encoding from NB1 to confirm the coversion of the diagnosis categorical data into numeric, where\n",
    "* Malignant = 1 (indicates prescence of cancer cells)\n",
    "* Benign = 0 (indicates abscence)\n",
    "\n",
    "#### **Observation**\n",
    "> *357 observations indicating the absence of cancer cells and 212 show absence of cancer cell*\n",
    "\n",
    "Lets confirm this, by ploting the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "821c5252-65f1-4cc2-be93-2dffa1fe7707"
    }
   },
   "source": [
    "# 2.3 Unimodal Data Visualizations\n",
    "\n",
    "One of the main goals of visualizing the data here is to observe which features are most helpful in predicting malignant or benign cancer. The other is to see general trends that may aid us in model selection and hyper parameter selection.\n",
    "\n",
    "Apply 3 techniques that you can use to understand each attribute of your dataset independently.\n",
    "* Histograms.\n",
    "* Density Plots.\n",
    "* Box and Whisker Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:49.181227Z",
     "start_time": "2021-12-06T08:41:48.719983Z"
    },
    "nbpresent": {
     "id": "fc8ea555-addd-449c-a8bb-cc0167a63d27"
    }
   },
   "outputs": [],
   "source": [
    "#lets get the frequency of cancer diagnosis\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context({\"figure.figsize\": (10, 8)})\n",
    "sns.countplot(df['diagnosis'],label='Count',palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7bf5e0ac-bb7c-4586-8cfb-3cd8c244ed73"
    }
   },
   "source": [
    "## 2.3.1 Visualise distribution of data via histograms\n",
    "Histograms are commonly used to visualize numerical variables. A histogram is similar to a bar graph after the values of the variable are grouped (binned) into a finite number of intervals (bins).\n",
    "\n",
    "Histograms group data into bins and provide you a count of the number of observations in each bin. From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed or even has an exponential distribution. It can also help you see possible outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f919d68b-1e2e-4f05-b3b3-c4684bcfb25e"
    }
   },
   "source": [
    "### Separate columns into smaller dataframes to perform visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:49.196220Z",
     "start_time": "2021-12-06T08:41:49.184225Z"
    },
    "nbpresent": {
     "id": "1aa3f2af-c89a-4d3b-a331-3440b3308d22"
    }
   },
   "outputs": [],
   "source": [
    "#Break up columns into groups, according to their suffix designation \n",
    "#(_mean, _se,and __worst) to perform visualisation plots off. \n",
    "#Join the 'ID' and 'Diagnosis' back on\n",
    "df_id_diag=df.loc[:,[\"id\",\"diagnosis\"]]\n",
    "df_diag=df.loc[:,[\"diagnosis\"]]\n",
    "\n",
    "#For a merge + slice:\n",
    "df_mean=df.iloc[:,1:11]\n",
    "df_se=df.iloc[:,11:22]\n",
    "df_worst=df.iloc[:,23:]\n",
    "\n",
    "print(df_id_diag.columns)\n",
    "#print(data_mean.columns)\n",
    "#print(data_se.columns)\n",
    "#print(data_worst.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "99300afc-baee-476f-afd2-4ff19fabc66b"
    }
   },
   "source": [
    "### Histogram the `_mean` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:52.090490Z",
     "start_time": "2021-12-06T08:41:49.198219Z"
    },
    "nbpresent": {
     "id": "560082ac-eddb-45bc-ad00-a7fe696552ef"
    }
   },
   "outputs": [],
   "source": [
    "#Plot histograms of CUT1 variables\n",
    "hist_mean=df_mean.hist(bins=10, figsize=(15, 10),grid=False,)\n",
    "\n",
    "#Any individual histograms, use this:\n",
    "#df_cut['radius_worst'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c9788ad7-eb08-48db-a654-bbf176cdaf58"
    }
   },
   "source": [
    "### Histogram for  the `_se` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:54.508469Z",
     "start_time": "2021-12-06T08:41:52.093486Z"
    },
    "nbpresent": {
     "id": "d2487c11-f386-43d2-a496-7fafab65accd"
    }
   },
   "outputs": [],
   "source": [
    "#Plot histograms of _se variables\n",
    "hist_se=df_se.hist(bins=10, figsize=(15, 10),grid=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "95aa2639-12dc-4cb6-b29f-5375ca8d6201"
    }
   },
   "source": [
    "### Histogram `_worst` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:56.680836Z",
     "start_time": "2021-12-06T08:41:54.510469Z"
    },
    "nbpresent": {
     "id": "93fe0c2d-4e9a-4368-a088-2e295b60730e"
    }
   },
   "outputs": [],
   "source": [
    "#Plot histograms of _worst variables\n",
    "hist_worst=df_worst.hist(bins=10, figsize=(15, 10),grid=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "31fa5bc8-b606-4282-9903-6a47c5e730f3"
    }
   },
   "source": [
    "### __Observation__ \n",
    "\n",
    ">We can see that perhaps the attributes  **concavity**,and **concavity_point ** may have an exponential distribution ( ). We can also see that perhaps the texture and smooth and symmetry attributes may have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c14f8b1e-5469-4e85-be8a-02266b2a80a5"
    }
   },
   "source": [
    "## 2.3.2 Visualize distribution of data via density plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "65751b9f-27c7-4508-bf7b-67c55de7dc27"
    }
   },
   "source": [
    "### Density plots `_mean` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:41:58.778116Z",
     "start_time": "2021-12-06T08:41:56.683833Z"
    },
    "nbpresent": {
     "id": "af37dac3-905c-46ce-82b3-bc780e63a73c"
    }
   },
   "outputs": [],
   "source": [
    "#Density Plots\n",
    "plt = df_mean.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n",
    "                     sharey=False, fontsize=12, figsize=(15,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e7e46fd3-cf56-4f35-89c2-a7e9226f4090"
    }
   },
   "source": [
    "### Density plots `_se` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:01.414583Z",
     "start_time": "2021-12-06T08:41:58.782076Z"
    },
    "nbpresent": {
     "id": "216093e9-63fc-4c36-a024-80f90cc1443e"
    }
   },
   "outputs": [],
   "source": [
    "#Density Plots\n",
    "plt = df_se.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n",
    "                    sharey=False, fontsize=12, figsize=(15,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b660469-510e-4cec-9a5e-fb79edb38749"
    }
   },
   "source": [
    "### Density plot `_worst` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:03.528318Z",
     "start_time": "2021-12-06T08:42:01.420583Z"
    },
    "nbpresent": {
     "id": "d63bd751-d2b0-44e2-9cee-739b94139e3e"
    }
   },
   "outputs": [],
   "source": [
    "#Density Plots\n",
    "plt = df_worst.plot(kind= 'kde', subplots=True, layout=(4,3), sharex=False, sharey=False, \n",
    "                    fontsize=5, figsize=(15,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "da731314-8bd8-452b-bfb2-34aea1b8ef13"
    }
   },
   "source": [
    "### Observation\n",
    ">We can see that perhaps the attributes perimeter,radius, area, concavity, compactness may have an exponential distribution( ). We can also see that perhaps the texture and smooth and symmetry attributes may have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0e9b89ea-19a5-46d9-b698-e8137db07ea4"
    }
   },
   "source": [
    "## 2.3.3 Visualise distribution of data via box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "71a739f0-3990-49c6-8fec-bc33b0bd9caa"
    }
   },
   "source": [
    "### Box plot `_mean` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:04.756812Z",
     "start_time": "2021-12-06T08:42:03.533313Z"
    },
    "nbpresent": {
     "id": "f758e15e-94d1-48d1-8881-6e914fd22d23"
    }
   },
   "outputs": [],
   "source": [
    "# box and whisker plots\n",
    "plt=df_mean.plot(kind= 'box' , subplots=True, layout=(4,4), sharex=False, sharey=False,\n",
    "                 fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbpresent": {
     "id": "af63ea0b-dff1-483f-bbee-4de09c8209f2"
    }
   },
   "source": [
    "### Box plot `_se` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:06.257534Z",
     "start_time": "2021-12-06T08:42:04.758811Z"
    },
    "nbpresent": {
     "id": "cc8fcba8-19c1-411b-80f8-9ef0168cf20b"
    }
   },
   "outputs": [],
   "source": [
    "# box and whisker plots\n",
    "plt=df_se.plot(kind= 'box' , subplots=True, layout=(4,4), sharex=False, sharey=False, \n",
    "               fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbpresent": {
     "id": "87596dd2-aff7-4b9d-a206-e2a921701c94"
    }
   },
   "source": [
    "### Box plot `_worst` suffix designition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:07.591467Z",
     "start_time": "2021-12-06T08:42:06.260534Z"
    },
    "nbpresent": {
     "id": "034a34da-81ed-4155-994b-d1bc17caf00f"
    }
   },
   "outputs": [],
   "source": [
    "# box and whisker plots\n",
    "plt=df_worst.plot(kind= 'box' , subplots=True, layout=(4,4), sharex=False, sharey=False, \n",
    "                  fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbpresent": {
     "id": "d57bf73f-ede9-4508-9e77-9417ec1e5855"
    }
   },
   "source": [
    "### Observation\n",
    ">We can see that perhaps the attributes perimeter,radius, area, concavity,ompactness may have an exponential distribution( ). We can also see that perhaps the texture and smooth and symmetry attributes may have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "582b6d03-3d98-42ee-a480-f999cd8042b7"
    }
   },
   "source": [
    "# 2.4 Multimodal Data Visualizations\n",
    "* Scatter plots\n",
    "* Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6f01572-5cb2-4a7f-b1cf-01fd88507e2d"
    }
   },
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:08.861630Z",
     "start_time": "2021-12-06T08:42:07.593463Z"
    },
    "nbpresent": {
     "id": "3b299da7-b9e0-42db-a2b7-40c70413fc25"
    }
   },
   "outputs": [],
   "source": [
    "# plot correlation matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "df = pd.read_csv('data/data_clean.csv', index_col=False)\n",
    "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df_mean.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "df, ax = plt.subplots(figsize=(8, 8))\n",
    "plt.title('Breast Cancer Feature Correlation')\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(260, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, vmax=1.2, square='square', cmap=cmap, mask=mask, \n",
    "            ax=ax,annot=True, fmt='.2g',linewidths=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "10e61694-c7f3-4267-a0e5-492e25e98a9c"
    }
   },
   "source": [
    "### Observation:\n",
    "We can see strong positive relationship exists with mean values paramaters between **1** to **0.75**.\n",
    "* The mean area of the tissue nucleus has a strong positive correlation with mean values of radius and parameter;\n",
    "* Some paramters are moderately positive corrlated (r between 0.5-0.75)are concavity and area, concavity and perimeter etc\n",
    "* Likewise, we see some strong negative correlation between fractal_dimension with radius, texture, parameter mean values.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:12.785085Z",
     "start_time": "2021-12-06T08:42:08.865628Z"
    },
    "nbpresent": {
     "id": "57bebe7b-a01b-4543-ad23-f5b62041e78a"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "df = pd.read_csv('data/data_clean.csv', index_col=False)\n",
    "g = sns.PairGrid(df[[df.columns[1],df.columns[2], df.columns[3],\n",
    "                     df.columns[4], df.columns[5], df.columns[6]]], hue='diagnosis')\n",
    "g = g.map_diag(plt.hist)\n",
    "g = g.map_offdiag(plt.scatter, s = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "02ff5f2d-748b-4d8f-929e-747ab2a7c7aa"
    }
   },
   "source": [
    "### Part_2 Summary:\n",
    "\n",
    "* Mean values of cell radius, perimeter, area, compactness, concavity\n",
    "    and concave points can be used in classification of the cancer. Larger\n",
    "    values of these parameters tends to show a correlation with malignant\n",
    "    tumors.\n",
    "* mean values of texture, smoothness, symmetry or fractual dimension\n",
    "    does not show a particular preference of one diagnosis over the other. \n",
    "    \n",
    "* In any of the histograms there are no noticeable large outliers that warrants further cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Part_3: Pre-Processing the data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[Data preprocessing](http://www.cs.ccsu.edu/~markov/ccsu_courses/datamining-3.html) is a crucial step for any data analysis problem.  It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use.This involves a number of activities such as:\n",
    "* Assigning numerical values to categorical data;\n",
    "* Handling missing values; and\n",
    "* Normalizing the features (so that features on small scales do not dominate when fitting a model to the data).\n",
    "\n",
    "In Part_2, I explored the data, to help gain insight on the distribution of the data as well as how the attributes correlate to each other. I identified some features of interest. In this notebook I use feature selection to reduce high-dimension data, feature extraction and transformation for dimensionality reduction. \n",
    "\n",
    "## Goal:\n",
    "Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. \n",
    "\n",
    "### Load data and essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:12.849065Z",
     "start_time": "2021-12-06T08:42:12.788083Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load libraries for data processing\n",
    "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4) \n",
    "#plt.rcParams['axes.titlesize'] = 'large'\n",
    "\n",
    "df = pd.read_csv('data/data_clean.csv', index_col=False)\n",
    "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "Here, I assign the 30 features to a NumPy array X, and transform the class labels from their original string representation (M and B) into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:12.893030Z",
     "start_time": "2021-12-06T08:42:12.852047Z"
    }
   },
   "outputs": [],
   "source": [
    "#Assign predictors to a variable of ndarray (matrix) type\n",
    "array = df.values\n",
    "X = array[:,1:31]\n",
    "y = array[:,0]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:15.199479Z",
     "start_time": "2021-12-06T08:42:12.896021Z"
    }
   },
   "outputs": [],
   "source": [
    "#transform the class labels from their original string representation (M and B) into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y\n",
    "\n",
    "# Call the transform method of LabelEncorder on two dummy variables\n",
    "# le.transform (['M', 'B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "> *After encoding the class labels(diagnosis) in an array y, the malignant tumors are now represented as class 1(i.e prescence of cancer cells) and the benign tumors are represented as class 0 (i.e no cancer cells detection), respectively*, illustrated by calling the transform method of LabelEncorder on two dummy variables.**\n",
    "\n",
    "\n",
    "### Assesing Model Accuracy: Split data into training and test sets\n",
    "\n",
    "The simplest method to evaluate the performance of a machine learning algorithm is to use different training and testing datasets. Here I will\n",
    "* Split the available data into a training set and a testing set. (70% training, 30% test)\n",
    "* Train the algorithm on the first part,\n",
    "* make predictions on the second part and \n",
    "* evaluate the predictions against the expected results. \n",
    "\n",
    "The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:15.293428Z",
     "start_time": "2021-12-06T08:42:15.201477Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##Split data set in train 70% and test 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=7)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Standardization\n",
    "\n",
    "* Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. \n",
    "\n",
    "* As seen in **Part_2** the raw data has differing distributions which may have an impact on the most ML algorithms. Most machine learning and optimization algorithms behave much better if features are on the same scale.\n",
    "\n",
    "Let’s evaluate the same algorithms with a standardized copy of the dataset. Here, I use sklearn to scale and transform the data such that each attribute has a mean value of zero and a standard deviation of one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:15.324414Z",
     "start_time": "2021-12-06T08:42:15.296424Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler =StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature decomposition using Principal Component Analysis (PCA)\n",
    "\n",
    "From the pair plot in **Part_2**, lot of feature pairs divide nicely the data to a similar extent, therefore, it makes sense to use one of the dimensionality reduction methods to try to use as many features as possible and maintian as much information as possible when working with only 2 dimensions. I will use PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:15.540288Z",
     "start_time": "2021-12-06T08:42:15.331409Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# feature extraction\n",
    "pca = PCA(n_components=10)\n",
    "fit = pca.fit(Xs)\n",
    "\n",
    "# summarize components\n",
    "#print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
    "#print(fit.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:15.866105Z",
     "start_time": "2021-12-06T08:42:15.543287Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pca = pca.transform(Xs)\n",
    "\n",
    "PCA_df = pd.DataFrame()\n",
    "\n",
    "PCA_df['PCA_1'] = X_pca[:,0]\n",
    "PCA_df['PCA_2'] = X_pca[:,1]\n",
    "\n",
    "plt.plot(PCA_df['PCA_1'][df.diagnosis == 'M'],PCA_df['PCA_2'][df.diagnosis == 'M'],'o', alpha = 0.7, color = 'r')\n",
    "plt.plot(PCA_df['PCA_1'][df.diagnosis == 'B'],PCA_df['PCA_2'][df.diagnosis == 'B'],'o', alpha = 0.7, color = 'b')\n",
    "\n",
    "plt.xlabel('PCA_1')\n",
    "plt.ylabel('PCA_2')\n",
    "plt.legend(['Malignant','Benign'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we got after applying the linear PCA transformation is a lower dimensional subspace (from 3D to 2D in this case), where the samples are **most spread** along the new feature axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:15.881114Z",
     "start_time": "2021-12-06T08:42:15.869105Z"
    }
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "#Cumulative Variance explains\n",
    "#var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "#print(var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding How Many Principal Components to Retain\n",
    "\n",
    "In order to decide how many principal components should be retained, it is common to summarise the results of a principal components analysis by making a scree plot. More about scree plot can be found **[here](http://python-for-multivariate-analysis.readthedocs.io/a_little_book_of_python_for_multivariate_analysis.html)**, and **[hear](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:16.284862Z",
     "start_time": "2021-12-06T08:42:15.883093Z"
    }
   },
   "outputs": [],
   "source": [
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "#Cumulative Variance explains\n",
    "#var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "#print(var1)\n",
    "\n",
    "plt.plot(var)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "\n",
    "leg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3, shadow=False, markerscale=0.4)\n",
    "leg.get_frame().set_alpha(0.4)\n",
    "leg.set_draggable(state=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "> ### Observation\n",
    "The most obvious change in slope in the scree plot occurs at component 2, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that the first three components should be retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part_3 Summary:  Data Preprocing Approach used\n",
    "\n",
    "1. assign features to a NumPy array X, and transform the class labels from their original string representation (M and B) into integers\n",
    "2. Split data into training and test sets\n",
    "3. Standardize the data.\n",
    "4. Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix\n",
    "5. Sort eigenvalues in descending order and choose the kk eigenvectors that correspond to the kk largest eigenvalues where k is the number of dimensions of the new feature subspace $(k≤dk≤d)$.\n",
    "6. Construct the projection matrix W from the selected k eigenvectors.\n",
    "7. Transform the original dataset X via W to obtain a k-dimensional feature subspace Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to select a subset of features that have the largest correlation with the class labels. The effect of feature selection must be assessed within a complete modeling pipeline in order to give you an unbiased estimated of your model's true performance. Hence, in the next section you will first be introduced to cross-validation, before applying the PCA-based feature selection strategy in the model building pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part_4: Predictive model using Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machines (SVMs) learning algorithm will be used to build the predictive model.  SVMs are one of the most popular classification algorithms, and have an elegant way of transforming nonlinear data so that one can use a linear algorithm to fit a linear model to the data (Cortes and Vapnik 1995).\n",
    "\n",
    "Kernelized support vector machines are powerful models and perform well on a variety of datasets. \n",
    "1. SVMs allow for complex decision boundaries, even if the data has only a few features. \n",
    "\n",
    "2. They work well on low-dimensional and high-dimensional data (i.e., few and many features), but don’t scale very well with the number of samples.\n",
    "> **Running an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.**\n",
    "\n",
    "3. SVMs requires careful preprocessing of the data and tuning of the parameters. This is why, these days, most people instead use tree-based models such as random forests or gradient boosting (which require little or no preprocessing) in many applications. \n",
    "\n",
    "4.  SVM models are hard to inspect; it can be difficult to understand why a particular prediction was made, and it might be tricky to explain the model to a nonexpert.\n",
    "\n",
    "### Important Parameters\n",
    "The important parameters in kernel SVMs are the\n",
    "* Regularization parameter C, \n",
    "* The choice of the kernel,(linear, radial basis function(RBF) or polynomial)\n",
    "* Kernel-specific parameters. \n",
    "\n",
    "gamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Therefore, good settings for the two parameters are usually strongly correlated, and C and gamma should be adjusted together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:16.315848Z",
     "start_time": "2021-12-06T08:42:16.287861Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load libraries for data processing\n",
    "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "## Supervised learning.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4) \n",
    "#plt.rcParams['axes.titlesize'] = 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:16.423523Z",
     "start_time": "2021-12-06T08:42:16.319845Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_clean.csv', index_col=False)\n",
    "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:16.531461Z",
     "start_time": "2021-12-06T08:42:16.426518Z"
    }
   },
   "outputs": [],
   "source": [
    "#Assign predictors to a variable of ndarray (matrix) type\n",
    "array = df.values\n",
    "X = array[:,1:31] # features\n",
    "y = array[:,0]\n",
    "\n",
    "#transform the class labels from their original string representation (M and B) into integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with cross-validation\n",
    "\n",
    "As discussed in **Part_3** splitting the data into test and training sets is crucial to avoid overfitting. This allows generalization of real, previously-unseen data. Cross-validation extends this idea further. Instead of having a single train/test split, we specify **so-called folds** so that the data is divided into similarly-sized folds. \n",
    "\n",
    "* Training occurs by taking all folds except one – referred to as the holdout sample. \n",
    "\n",
    "* On the completion of the training, you test the performance of your fitted model using the holdout sample. \n",
    "\n",
    "* The holdout sample is then thrown back with the rest of the other folds, and a different fold is pulled out as the new holdout sample. \n",
    "\n",
    "* Training is repeated again with the remaining folds and we measure performance using the holdout sample. This process is repeated until each fold has had a chance to be a test or holdout sample. \n",
    "\n",
    "* The expected performance of the classifier, called cross-validation error, is then simply an average of error rates computed on each holdout sample. \n",
    "\n",
    "This process is demonstrated by first performing a standard train/test split, and then computing cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:16.670439Z",
     "start_time": "2021-12-06T08:42:16.534457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Divide records in training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=2, stratify=y)\n",
    "\n",
    "# Create an SVM classifier and train it on 70% of the data set.\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Analyze accuracy of predictions on 30% of the holdout test sample.\n",
    "classifier_score = clf.score(X_test, y_test)\n",
    "print ('\\n➔ The classifier accuracy score is {:03.2f}\\n'.format(classifier_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better measure of prediction accuracy (which you can use as a proxy for “goodness of fit” of the model), you can successively split the data into folds that you will use for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:16.824884Z",
     "start_time": "2021-12-06T08:42:16.674436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get average of 3-fold cross-validation score using an SVC estimator.\n",
    "n_folds = 3\n",
    "cv_error = np.average(cross_val_score(SVC(), Xs, y, cv=n_folds))\n",
    "print ('\\n➔ The {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above evaluations were based on using the entire set of features. You will now employ the correlation-based feature selection strategy to assess the effect of using 3 features which have the best correlation with the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:17.056751Z",
     "start_time": "2021-12-06T08:42:16.827881Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "clf2 = make_pipeline(SelectKBest(f_regression, k=3),SVC(probability=True))\n",
    "\n",
    "scores = cross_val_score(clf2, Xs, y, cv=3)\n",
    "\n",
    "# Get average of 3-fold cross-validation score using an SVC estimator.\n",
    "n_folds = 3\n",
    "cv_error = np.average(cross_val_score(SVC(), Xs, y, cv=n_folds))\n",
    "print ('\\n➔ The {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:17.071742Z",
     "start_time": "2021-12-06T08:42:17.059748Z"
    }
   },
   "outputs": [],
   "source": [
    "print (scores)\n",
    "avg = (100*np.mean(scores), 100*np.std(scores)/np.sqrt(scores.shape[0]))\n",
    "print (\"➔ Average score and uncertainty: (%.2f +- %.3f)%%\"%avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, you can see that only a fraction of the features are required to build a model that performs similarly to models based on using the entire set of features.\n",
    "Feature selection is an important part of the model-building process that you must always pay particular attention to. The details are beyond the scope of this notebook. In the rest of the analysis, you will continue using the entire set of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Model Accuracy: Receiver Operating Characteristic (ROC) curve\n",
    "\n",
    "In statistical modeling and machine learning, a commonly-reported performance measure of model accuracy for binary classification problems is Area Under the Curve (AUC).\n",
    "\n",
    "To understand what information the ROC curve conveys, consider the so-called confusion matrix that essentially is a two-dimensional table where the classifier model is on one axis (vertical), and ground truth is on the other (horizontal) axis, as shown below. Either of these axes can take two values (as depicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    " Model says \"+\" |Model says  \"-\" \n",
    "--- | --- | ---\n",
    "`True positive` | `False negative` | ** Actual: \"+\" **\n",
    " `False positive`  | `True negative` |  Actual: \"-\"\n",
    " In an ROC curve, you plot “True Positive Rate” on the Y-axis and “False Positive Rate” on the X-axis, where the values “true positive”, “false negative”, “false positive”, and “true negative” are events (or their probabilities) as described above. The rates are defined according to the following:\n",
    "> * True positive rate (or sensitivity)}: tpr = tp / (tp + fn)\n",
    "> * False positive rate:       fpr = fp / (fp + tn)\n",
    "> * True negative rate (or specificity): tnr = tn / (fp + tn)\n",
    "\n",
    "In all definitions, the denominator is a row margin in the above confusion matrix. Thus,one can  express\n",
    "* the true positive rate (tpr) as the probability that the model says \"+\" when the real value is indeed \"+\" (i.e., a conditional probability). However, this does not tell you how likely you are to be correct when calling \"+\" (i.e., the probability of a true positive, conditioned on the test result being \"+\").\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:17.212062Z",
     "start_time": "2021-12-06T08:42:17.074741Z"
    }
   },
   "outputs": [],
   "source": [
    "# The confusion matrix helps visualize the performance of the algorithm.\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "#print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:17.474343Z",
     "start_time": "2021-12-06T08:42:17.215061Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values')\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation \n",
    "There are two possible predicted classes: \"1\" and \"0\". Malignant = 1 (indicates prescence of cancer cells) and Benign\n",
    "= 0 (indicates abscence).\n",
    "\n",
    "* The classifier made a total of 174 predictions (i.e 174 patients were being tested for the presence breast cancer).\n",
    "* Out of those 174 cases, the classifier predicted \"yes\" 58 times, and \"no\" 113 times.\n",
    "* In reality, 64 patients in the sample have the disease, and 107 patients do not.\n",
    "\n",
    "#### Rates as computed from the confusion matrix\n",
    "1. **Accuracy**: Overall, how often is the classifier correct?\n",
    "    * (TP+TN)/total = (57+106)/171 = 0.95\n",
    "\n",
    "2. **Misclassification Rate**: Overall, how often is it wrong?\n",
    "    * (FP+FN)/total = (1+7)/171 = 0.05 equivalent to 1 minus Accuracy also known as ***\"Error Rate\"***\n",
    "\n",
    "3. **True Positive Rate:** When it's actually yes, how often does it predict 1?\n",
    "   * TP/actual yes = 57/64 = 0.89 also known as \"Sensitivity\" or ***\"Recall\"***\n",
    "\n",
    "4. **False Positive Rate**: When it's actually 0, how often does it predict 1?\n",
    "   * FP/actual no = 1/107 = 0.01\n",
    "\n",
    "5. **Specificity**: When it's actually 0, how often does it predict 0? also know as **true positive rate**\n",
    "   * TN/actual no = 106/107 = 0.99 equivalent to 1 minus False Positive Rate\n",
    "\n",
    "6. **Precision**: When it predicts 1, how often is it correct?\n",
    "   * TP/predicted yes = 57/58 = 0.98\n",
    "\n",
    "7. **Prevalence**: How often does the yes condition actually occur in our sample?\n",
    "   * actual yes/total = 64/171 = 0.34\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:17.876727Z",
     "start_time": "2021-12-06T08:42:17.477338Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot the receiver operating characteristic curve (ROC).\n",
    "plt.figure(figsize=(10,8))\n",
    "probas_ = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.axes().set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To interpret the ROC correctly, consider what the points that lie along the diagonal represent. For these situations, there is an equal chance of \"+\" and \"-\" happening. Therefore, this is not that different from making a prediction by tossing of an unbiased coin. Put simply, the classification model is random.\n",
    "\n",
    "* For the points above the diagonal, tpr > fpr, and the model says that you are in a zone where you are performing better than random. For example, assume tpr = 0.99 and fpr = 0.01, Then, the probability of being in the true positive group is $(0.99 / (0.99 + 0.01)) = 99\\%$. Furthermore, holding fpr constant, it is easy to see that the more vertically above the diagonal you are positioned, the better the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we'll look into optimizing the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Cortes, Corinna, and Vladimir Vapnik. 1995. 'Support-Vector Networks.' Machine Learning 20: 273. Accessed September 3, 2016. doi: 10.1023/A:1022627411411](https://link.springer.com/article/10.1007/BF00994018)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part_5: Optimizing the SVM Classifier\n",
    "\n",
    "Machine learning models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this part, we'll aim to tune parameters of the SVM Classification model using scikit-learn. \n",
    "\n",
    "### Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:17.908705Z",
     "start_time": "2021-12-06T08:42:17.879725Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load libraries for data processing\n",
    "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "## Supervised learning.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4) \n",
    "#plt.rcParams['axes.titlesize'] = 'large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a predictive model and evaluate with 5-cross validation using support vector classifies (ref Part_4) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:18.031442Z",
     "start_time": "2021-12-06T08:42:17.914706Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:18.556905Z",
     "start_time": "2021-12-06T08:42:18.034441Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_clean.csv', index_col=False)\n",
    "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "# Assign predictors to a variable of ndarray (matrix) type\n",
    "array = df.values\n",
    "X = array[:,1:31]\n",
    "y = array[:,0]\n",
    "\n",
    "# Transform the class labels from their original string representation (M and B) into integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler =StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# feature extraction\n",
    "pca = PCA(n_components=10)\n",
    "fit = pca.fit(Xs)\n",
    "X_pca = pca.transform(Xs)\n",
    "\n",
    "# Divide records in training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=2, stratify=y)\n",
    "\n",
    "# Create an SVM classifier and train it on 70% of the data set.\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Analyze accuracy of predictions on 30% of the holdout test sample.\n",
    "classifier_score = clf.score(X_test, y_test)\n",
    "print ('\\nThe classifier accuracy score is {:03.2f}\\n'.format(classifier_score))\n",
    "\n",
    "clf2 = make_pipeline(SelectKBest(f_regression, k=3),SVC(probability=True))\n",
    "scores = cross_val_score(clf2, X_pca, y, cv=3)\n",
    "\n",
    "# Get average of 5-fold cross-validation score using an SVC estimator.\n",
    "n_folds = 5\n",
    "cv_error = np.average(cross_val_score(SVC(), X_pca, y, cv=n_folds))\n",
    "#print ('\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error))\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred ))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of optimizing a classifier\n",
    "\n",
    "We can tune two key parameters of the SVM algorithm:\n",
    "* the value of C (how much to relax the margin) \n",
    "* and the type of kernel. \n",
    "\n",
    "The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid search using 10-fold cross validation with a standardized copy of the training dataset. We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively).\n",
    "\n",
    "Python scikit-learn provides two simple methods for algorithm parameter tuning:\n",
    " * Grid Search Parameter Tuning. \n",
    " * Random Search Parameter Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:24.424599Z",
     "start_time": "2021-12-06T08:42:18.559901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train classifiers.\n",
    "kernel_values = [ 'linear' ,  'poly' ,  'rbf' ,  'sigmoid' ]\n",
    "param_grid = {'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6),'kernel': kernel_values}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:24.439572Z",
     "start_time": "2021-12-06T08:42:24.429578Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:24.595489Z",
     "start_time": "2021-12-06T08:42:24.443570Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_.probability = True\n",
    "clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:25.211514Z",
     "start_time": "2021-12-06T08:42:24.599481Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "#print(cm)\n",
    "print(classification_report(y_test, y_pred ))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Decision boundaries of different classifiers\n",
    "\n",
    "Let's see the decision boundaries produced by the linear, Gaussian and polynomial classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:25.463016Z",
     "start_time": "2021-12-06T08:42:25.215023Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "def decision_plot(X_train, y_train, n_neighbors, weights):\n",
    "       h = .02  # step size in the mesh\n",
    "\n",
    "Xtrain = X_train[:, :2] # we only take the first two features.\n",
    "\n",
    "#================================================================\n",
    "# Create color maps\n",
    "#================================================================\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "\n",
    "#================================================================\n",
    "# we create an instance of SVM and fit out data. \n",
    "# We do not scale ourdata since we want to plot the support vectors\n",
    "#================================================================\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=0, gamma=0.1, C=C).fit(Xtrain, y_train)\n",
    "rbf_svc = SVC(kernel='rbf', gamma=0.7, C=C).fit(Xtrain, y_train)\n",
    "poly_svc = SVC(kernel='poly', degree=3, C=C).fit(Xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:25.493507Z",
     "start_time": "2021-12-06T08:42:25.466523Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 9) \n",
    "plt.rcParams['axes.titlesize'] = 'large'\n",
    "    \n",
    "    # create a mesh to plot in\n",
    "x_min, x_max = Xtrain[:, 0].min() - 1, Xtrain[:, 0].max() + 1\n",
    "y_min, y_max = Xtrain[:, 1].min() - 1, Xtrain[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:42:27.773102Z",
     "start_time": "2021-12-06T08:42:25.497504Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, clf in enumerate((svm, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=y_train, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('radius_mean')\n",
    "    plt.ylabel('texture_mean')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This work demonstrates the modelling of breast cancer as classification task using Support Vector Machine \n",
    "\n",
    "The SVM performs better when the dataset is standardized so that all attributes have a mean value of zero and a standard deviation of one. We can calculate this from the entire training dataset and apply the same transform to the input attributes from the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Task:\n",
    "\n",
    "1. Summary and conclusion of findings\n",
    "2. Compare with other classification methods\n",
    "   * Decision trees with **`tree.DecisionTreeClassifier()`**\n",
    "   * K-nearest neighbors with **`neighbors.KNeighborsClassifier()`**\n",
    "   * Random forests with **`ensemble.RandomForestClassifier()`**\n",
    "   * Perceptron (both gradient and stochastic gradient) with **`mlxtend.classifier.Perceptron`**\n",
    "   * Multilayer perceptron network (both gradient and stochastic gradient) with **`mlxtend.classifier.MultiLayerPerceptron`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b1daa992-4a28-496f-b325-6e8be1a7844c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part_6: Automate the ML process using pipelines \n",
    "\n",
    "There are standard workflows in a machine learning project that can be automated. In Python scikit-learn, Pipelines help to clearly define and automate these workflows.\n",
    "* Pipelines help overcome common problems like data leakage in your test harness. \n",
    "* Python scikit-learn provides a Pipeline utility to help automate machine learning workflows.\n",
    "* Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated.\n",
    "\n",
    "## Data Preparation and Modeling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:50:56.938007Z",
     "start_time": "2021-12-06T08:50:52.772675Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pipeline that standardizes the data then creates a model\n",
    "#Load libraries for data processing\n",
    "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4) \n",
    "#plt.rcParams['axes.titlesize'] = 'large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Some Algorithms\n",
    "Now it is time to create some models of the data and estimate their accuracy on unseen data. Here is what we are going to cover in this step:\n",
    "1. Separate out a validation dataset.\n",
    "2. Setup the test harness to use 10-fold cross validation.\n",
    "3. Build 5 different models  \n",
    "4. Select the best model\n",
    "\n",
    "## Dataset Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:50:58.659367Z",
     "start_time": "2021-12-06T08:50:58.613424Z"
    }
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('data/data_clean.csv', index_col=False)\n",
    "df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "# Split-out validation dataset\n",
    "array = df.values\n",
    "X = array[:,1:31]\n",
    "y = array[:,0]\n",
    "\n",
    "# Divide records in training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "\n",
    "#transform the class labels from their original string representation (M and B) into integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Algorithms: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:51:01.066996Z",
     "start_time": "2021-12-06T08:51:00.310370Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "seed = 7 \n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "seed = 7 \n",
    "scoring = 'accuracy'\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "print('\\n➔ 10-Fold cross-validation accurcay score for the training data for six classifiers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:51:04.475636Z",
     "start_time": "2021-12-06T08:51:04.453982Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Observation\n",
    "> The results suggest That both Logistic Regression and LDA may be worth further study. These are just mean accuracy values. It is always wise to look at the distribution of accuracy values calculated across cross validation folds. We can do that graphically using box and whisker plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:51:06.253307Z",
     "start_time": "2021-12-06T08:51:05.938373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "> The results show a similar tight distribution for all classifiers except SVM which is encouraging, suggesting low variance. The good results for SVM are satisfactory.\n",
    "\n",
    "> It is possible the varied distribution of the attributes may have an effect on the accuracy of algorithms such as SVM. In the next section we will repeat this spot-check with a standardized copy of the training dataset.\n",
    "\n",
    "\n",
    "## 2.1 Evaluate Algorithms: Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:51:08.553833Z",
     "start_time": "2021-12-06T08:51:07.767018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:  \n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold,\n",
    "      scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:51:11.711001Z",
     "start_time": "2021-12-06T08:51:11.317249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Scaled Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "> The results show that standardization of the data has lifted the skill of SVM to be the most accurate algorithm tested so far.\n",
    "\n",
    "> The results suggest digging deeper into the SVM and LDA and LR algorithms. It is very likely that configuration beyond the default may yield even more accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Algorithm Tuning\n",
    "In this section we investigate tuning the parameters for three algorithms that show promise from the spot-checking in the previous section: LR, LDA and SVM.\n",
    "\n",
    "### Tuning hyper-parameters - SVC estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:52:02.775423Z",
     "start_time": "2021-12-06T08:51:14.146635Z"
    }
   },
   "outputs": [],
   "source": [
    "#Make Support Vector Classifier Pipeline\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=2)),\n",
    "                     ('clf', SVC(probability=True, verbose=False))])\n",
    "\n",
    "#Fit Pipeline to training Data\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "#print('➝ Fitted Pipeline to training Data')\n",
    "\n",
    "scores = cross_val_score(estimator=pipe_svc, X=X_train, y=y_train, cv=10, n_jobs=1, verbose=0)\n",
    "print('➔ Model Training Accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'clf__C': param_range,'clf__kernel': ['linear']},\n",
    "              {'clf__C': param_range,'clf__gamma': param_range,\n",
    "               'clf__kernel': ['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10,\n",
    "                  n_jobs=1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print('➔ Tuned Parameters Best Score: ',gs.best_score_)\n",
    "print('➔ Best Parameters: \\n',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyper-parameters: k-NN hyperparameters\n",
    "\n",
    " For your standard k-NN implementation, there are two primary hyperparameters that you’ll want to tune:\n",
    "\n",
    "* The number of neighbors k.\n",
    "* The distance metric/similarity function.\n",
    "\n",
    "Both of these values can dramatically affect the accuracy of your k-NN classifier. Grid object is ready to do 10-fold cross validation on a KNN model using classification accuracy as the evaluation metric\n",
    "In addition, there is a parameter grid to repeat the 10-fold cross validation process 30 times\n",
    "Each time, the n_neighbors parameter should be given a different value from the list\n",
    "We can't give **`GridSearchCV`** just a list\n",
    "We've to specify n_neighbors should take on 1 through 30\n",
    "You can set **`n_jobs = -1`** to run computations in parallel (if supported by your computer and OS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:52:11.929625Z",
     "start_time": "2021-12-06T08:52:08.963196Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "pipe_knn = Pipeline([('scl', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=2)),\n",
    "                     ('clf', KNeighborsClassifier())])\n",
    "            \n",
    "#Fit Pipeline to training Data\n",
    "pipe_knn.fit(X_train, y_train) \n",
    "\n",
    "scores = cross_val_score(estimator=pipe_knn, \n",
    "                         X=X_train, \n",
    "                         y=y_train, \n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "print('➝ Model Training Accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n",
    "\n",
    "#Tune Hyperparameters\n",
    "param_range = range(1, 31)\n",
    "param_grid = [{'clf__n_neighbors': param_range}]\n",
    "# instantiate the grid\n",
    "gs = GridSearchCV(estimator=pipe_knn, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=10, \n",
    "                    scoring='accuracy')\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print('➔ Tuned Parameters Best Score: ',gs.best_score_)\n",
    "print('➔ Best Parameters: \\n',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:52:15.094216Z",
     "start_time": "2021-12-06T08:52:14.926535Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use best parameters\n",
    "clf_svc = gs.best_estimator_\n",
    "\n",
    "#Get Final Scores\n",
    "clf_svc.fit(X_train, y_train)\n",
    "scores = cross_val_score(estimator=clf_svc,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "\n",
    "print('➔ Final Model Training Accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))\n",
    "print('➜ Final Accuracy on Test set: %.5f' % clf_svc.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T08:52:22.748973Z",
     "start_time": "2021-12-06T08:52:22.697611Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_svc.fit(X_train, y_train)\n",
    "y_pred = clf_svc.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Worked through a classification predictive modeling machine learning problem from end-to-end using Python. Specifically, the steps covered were:\n",
    "1. Problem Definition (Breast Cancer data).\n",
    "2. Loading the Dataset.\n",
    "3. Analyze Data (same scale but di↵erent distributions of data).\n",
    "    * Evaluate Algorithms (KNN looked good).\n",
    "    * Evaluate Algorithms with Standardization (KNN and SVM looked good).\n",
    "4. Algorithm Tuning (K=19 for KNN was good, SVM with an RBF kernel and C=100 was best).. \n",
    "5. Finalize Model (use all training data and confirm using validation dataset)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "nbpresent": {
   "slides": {
    "029765a0-324f-4829-89ed-d4c3d7bdfa56": {
     "id": "029765a0-324f-4829-89ed-d4c3d7bdfa56",
     "prev": "eb53caf1-dd5e-40f6-a08e-acb2c9064355",
     "regions": {
      "020ba674-4256-4ab4-93a1-7e0e7a2161ba": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e6f8be49-68cc-4381-9b0f-c872712045b6",
        "part": "whole"
       },
       "id": "020ba674-4256-4ab4-93a1-7e0e7a2161ba"
      }
     }
    },
    "17063b65-52ab-4296-866f-1732fbe2b4c5": {
     "id": "17063b65-52ab-4296-866f-1732fbe2b4c5",
     "prev": "c5378b35-bbe4-43b7-9739-6d787ad32ada",
     "regions": {
      "a2944b54-cb8b-4d2d-9a89-e0fdfc9a5a03": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b6af97c7-30d4-4fc6-b9d5-a213f70788c2",
        "part": "whole"
       },
       "id": "a2944b54-cb8b-4d2d-9a89-e0fdfc9a5a03"
      }
     }
    },
    "1ce04cdb-6165-4b4f-aaf7-e856a34a5139": {
     "id": "1ce04cdb-6165-4b4f-aaf7-e856a34a5139",
     "prev": "3ae68135-1f69-426a-baea-d9929a65e1c1",
     "regions": {
      "26b78508-b18f-4e94-9db0-4e860e9aeef3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "73ab9300-46cb-4deb-a673-7b6f0ea718bc",
        "part": "whole"
       },
       "id": "26b78508-b18f-4e94-9db0-4e860e9aeef3"
      }
     }
    },
    "364cf224-3a9c-4786-a47f-522b3613c613": {
     "id": "364cf224-3a9c-4786-a47f-522b3613c613",
     "prev": "58d1aade-0c9f-4b85-8ac1-1ce8631623e8",
     "regions": {
      "6996a269-8482-407e-8720-5ae4f5c7dbd2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "06fb5119-7446-4892-a2a5-4770b4f98297",
        "part": "whole"
       },
       "id": "6996a269-8482-407e-8720-5ae4f5c7dbd2"
      }
     }
    },
    "3ae68135-1f69-426a-baea-d9929a65e1c1": {
     "id": "3ae68135-1f69-426a-baea-d9929a65e1c1",
     "prev": "e7e979eb-a900-4dea-b580-ce7a8750f2ed",
     "regions": {
      "bd9adb8d-a5a5-4462-bf77-b75de0851d0a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "65d636e8-6dca-4a24-988f-10dac2b05134",
        "part": "whole"
       },
       "id": "bd9adb8d-a5a5-4462-bf77-b75de0851d0a"
      }
     }
    },
    "504c799a-10a2-49b6-84c5-f8183c00d180": {
     "id": "504c799a-10a2-49b6-84c5-f8183c00d180",
     "prev": "f110b360-85b7-4b4b-b99f-6d7647b3bed6",
     "regions": {
      "cc92786c-5118-4036-84aa-87fa91d28467": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3749cd67-821c-4c70-b6e1-c9fa76be4ee8",
        "part": "whole"
       },
       "id": "cc92786c-5118-4036-84aa-87fa91d28467"
      }
     }
    },
    "58d1aade-0c9f-4b85-8ac1-1ce8631623e8": {
     "id": "58d1aade-0c9f-4b85-8ac1-1ce8631623e8",
     "prev": "d46dbd63-6e42-4de8-a34a-d797e2bf75a2",
     "regions": {
      "2814a179-b51d-4b1e-b1ea-05830fdc84ff": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a043ba25-f180-41cf-b29b-ec7e6bab8d57",
        "part": "whole"
       },
       "id": "2814a179-b51d-4b1e-b1ea-05830fdc84ff"
      }
     }
    },
    "8752525b-c794-48fe-8aa7-d4a85bdf53fa": {
     "id": "8752525b-c794-48fe-8aa7-d4a85bdf53fa",
     "prev": "df830ab6-e594-43d4-a9ad-f241e13b1278",
     "regions": {
      "7226298c-bfa5-44a2-bd6d-21fb50409904": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "678c0d45-faba-4084-9235-0923c653792a",
        "part": "whole"
       },
       "id": "7226298c-bfa5-44a2-bd6d-21fb50409904"
      }
     }
    },
    "92de18eb-29ed-44b0-83c8-95db511e0e55": {
     "id": "92de18eb-29ed-44b0-83c8-95db511e0e55",
     "prev": "8752525b-c794-48fe-8aa7-d4a85bdf53fa",
     "regions": {
      "9f8e4fc0-1a4c-4f98-964f-9027dcfb6df5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0a72bf45-3489-4633-a99f-d4fba5d072ac",
        "part": "whole"
       },
       "id": "9f8e4fc0-1a4c-4f98-964f-9027dcfb6df5"
      }
     }
    },
    "98ca86b5-1929-489d-bc0c-5095a542b89f": {
     "id": "98ca86b5-1929-489d-bc0c-5095a542b89f",
     "prev": null,
     "regions": {
      "4080ad88-0b11-4a89-a777-7a22dcc73104": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9f1df27f-184a-4b8b-bac6-951c73073085",
        "part": "whole"
       },
       "id": "4080ad88-0b11-4a89-a777-7a22dcc73104"
      }
     }
    },
    "98e9ac6a-316b-4e29-b4ee-bc6a149ceffb": {
     "id": "98e9ac6a-316b-4e29-b4ee-bc6a149ceffb",
     "prev": "c8da1fba-4fc5-4f36-8fac-423cbb55f44a",
     "regions": {
      "4cd7cf6b-40c0-4886-a5cc-a8821d855a7b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "818004ce-3b93-48be-9fb8-9bacb0f07f33",
        "part": "whole"
       },
       "id": "4cd7cf6b-40c0-4886-a5cc-a8821d855a7b"
      }
     }
    },
    "adcf599c-e7e7-4245-b171-d38d06a41644": {
     "id": "adcf599c-e7e7-4245-b171-d38d06a41644",
     "prev": "d93546de-291d-4547-8f16-425c2edc33ee",
     "regions": {
      "342e1946-4183-4572-994d-5cc77c8b1ab6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a7e09e9f-7609-4cb0-b81e-67442d426e0d",
        "part": "whole"
       },
       "id": "342e1946-4183-4572-994d-5cc77c8b1ab6"
      }
     }
    },
    "c5378b35-bbe4-43b7-9739-6d787ad32ada": {
     "id": "c5378b35-bbe4-43b7-9739-6d787ad32ada",
     "prev": "98e9ac6a-316b-4e29-b4ee-bc6a149ceffb",
     "regions": {
      "2b8bbb2a-fa7f-4a72-80ce-f5deb38e8b22": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "34efc993-87f8-4055-bde3-0231aacc0579",
        "part": "whole"
       },
       "id": "2b8bbb2a-fa7f-4a72-80ce-f5deb38e8b22"
      }
     }
    },
    "c8da1fba-4fc5-4f36-8fac-423cbb55f44a": {
     "id": "c8da1fba-4fc5-4f36-8fac-423cbb55f44a",
     "prev": "adcf599c-e7e7-4245-b171-d38d06a41644",
     "regions": {
      "5acfa550-6a81-4477-afe6-a6d1de5d5fb4": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e7a5c9bd-eaa8-4f5e-9e19-18ceea5e11af",
        "part": "whole"
       },
       "id": "5acfa550-6a81-4477-afe6-a6d1de5d5fb4"
      }
     }
    },
    "d46dbd63-6e42-4de8-a34a-d797e2bf75a2": {
     "id": "d46dbd63-6e42-4de8-a34a-d797e2bf75a2",
     "prev": "1ce04cdb-6165-4b4f-aaf7-e856a34a5139",
     "regions": {
      "993139e0-e0c0-4d94-a67a-8d83eb3ab97b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5f0c338b-0d11-4d51-836a-e003c07412ee",
        "part": "whole"
       },
       "id": "993139e0-e0c0-4d94-a67a-8d83eb3ab97b"
      }
     }
    },
    "d93546de-291d-4547-8f16-425c2edc33ee": {
     "id": "d93546de-291d-4547-8f16-425c2edc33ee",
     "prev": "98ca86b5-1929-489d-bc0c-5095a542b89f",
     "regions": {
      "a9485a01-dd0d-4ca2-a9a4-4dfa70d762af": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f9774957-e9a8-4e5d-a3a2-11d58c98f4f3",
        "part": "whole"
       },
       "id": "a9485a01-dd0d-4ca2-a9a4-4dfa70d762af"
      }
     }
    },
    "df830ab6-e594-43d4-a9ad-f241e13b1278": {
     "id": "df830ab6-e594-43d4-a9ad-f241e13b1278",
     "prev": "e8f59263-df5b-41b7-b47a-b72401367b8c",
     "regions": {
      "539907c0-857c-49ab-a35b-689088d4d441": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "dd80c7bb-5dfd-4bac-b1f9-92f09954a04d",
        "part": "whole"
       },
       "id": "539907c0-857c-49ab-a35b-689088d4d441"
      }
     }
    },
    "e7e979eb-a900-4dea-b580-ce7a8750f2ed": {
     "id": "e7e979eb-a900-4dea-b580-ce7a8750f2ed",
     "prev": "029765a0-324f-4829-89ed-d4c3d7bdfa56",
     "regions": {
      "ef9a6766-0a12-4eb5-bb55-c45a7807f03e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a9522023-ad9b-4f5b-bc07-36ff15f1746a",
        "part": "whole"
       },
       "id": "ef9a6766-0a12-4eb5-bb55-c45a7807f03e"
      }
     }
    },
    "e8f59263-df5b-41b7-b47a-b72401367b8c": {
     "id": "e8f59263-df5b-41b7-b47a-b72401367b8c",
     "prev": "17063b65-52ab-4296-866f-1732fbe2b4c5",
     "regions": {
      "7f5be598-0698-4492-b474-a68934db4693": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3411a17b-52dd-4b3d-8ec4-22e6ea79a516",
        "part": "whole"
       },
       "id": "7f5be598-0698-4492-b474-a68934db4693"
      }
     }
    },
    "eb53caf1-dd5e-40f6-a08e-acb2c9064355": {
     "id": "eb53caf1-dd5e-40f6-a08e-acb2c9064355",
     "prev": "92de18eb-29ed-44b0-83c8-95db511e0e55",
     "regions": {
      "0e9f4ac0-108d-4a56-8920-34eaa676bbd6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "01bd515a-1951-4ae7-8d39-a96079fc9eff",
        "part": "whole"
       },
       "id": "0e9f4ac0-108d-4a56-8920-34eaa676bbd6"
      }
     }
    },
    "f110b360-85b7-4b4b-b99f-6d7647b3bed6": {
     "id": "f110b360-85b7-4b4b-b99f-6d7647b3bed6",
     "prev": "364cf224-3a9c-4786-a47f-522b3613c613",
     "regions": {
      "95a79cf2-40d9-401a-9392-2dff5250fc23": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9469d4ac-470e-4b21-a460-bdfb54fd0d4f",
        "part": "whole"
       },
       "id": "95a79cf2-40d9-401a-9392-2dff5250fc23"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
